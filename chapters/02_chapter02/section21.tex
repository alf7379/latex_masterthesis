% !TeX root = ../../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\section{Basics}\label{ord:ch2:sec1}

In the last decade the development and research interest in the field of \gls{ml} have greatly increased and drawn much attention. 
This section briefly recaptures the fundamental concepts of \gls{ml} and \gls{dl}.
A detailed description of the introduced concepts and terms can be reviewed in \cite{Ger17-HandsOn} or \cite{Goodfellow-et-al-2016}.

%The last decade was revolutionary for the sector of information technology.
%Due to technical advancement, the computational power of processors, especially \gls{gpu} rise significantly.
%Further, the wider creation and use of data introduced the domain of big data, that allows operators to gain more insights and benefits.
%Both of these recent advancements benefited another field of study commonly described as \gls{ai}.
%The term \gls{ai} describes machines that are show characteristics of human intelligence that allow them to handle various tasks.
%But there are several gradations, that hide behind the powerful bus word \gls{ai} and are illustrated in this section.

\subsection{Machine Learning}\label{ord:ch2:sec1:subsec1}

\gls{ml} is the science of creating algorithms, to analyze, detect and learn patterns in various kinds of data without explicitly being programming.
These algorithms may achieve human level performance, but mostly they are limited to a single task.
In order to solve another task a \gls{ml} algorithm, has to learn the patterns of the new task from a corresponding dataset $\mathcal{D}$.
\gls{ml} algorithms are referred to as models.

% Dataset 
The dataset $\mathcal{D}$ usually is split into a training set $\mathcal{S}$, a test set $\mathcal{S}_2$ and validation set $\mathcal{S}_3$.
These three types of datasets are treated separately, while the model uses the training set $\mathcal{S}$ to extract the patterns, the test set $\mathcal{S}_2$ and validation set $\mathcal{S}_3$ are only used to evaluate the performance of the model.

% Input x and label y
A dataset $\mathcal{D}$ contains $N$ elements with each element having $M$ features.
An element $\textbf{x}$ can be represented as feature-vector $\textbf{x} = [x_1, \dots ,x_M]$, which the model takes as input.
For the task of supervised learning, the dataset $\mathcal{D}$ consists out of pairs $(\textbf{x}_n, y_n)$. Each input $\textbf{x}_i$ contains a corresponding label $y_i$.
In the scope of classification, this label $y$ is represented by one out of $K$ classes $y \in \lbrace c_1, \dots, c_K \rbrace$.
The classification task is defined as  
\begin{equation}
	f: \mathbb{R}^M \rightarrow \lbrace c_1,\ldots,c_K \rbrace, \quad \textbf{x} \mapsto y
\end{equation}
In order to create a functioning model, the model is trained by adjusting it's inner parameters, referred to as the weights of the model.
While training a model, the weights are adjusted based on the features are extracted iteratively from the given input $\textbf{x}$.

% Model prediction
The function $f$ describes the model, which computes the estimated output $\hat{y}$ based on the input $\textbf{x}$
\begin{equation}
	\hat y = f(\textbf{x}, \theta)
\end{equation}
with $\hat{y}$ being also referred to as prediction of the model.

% Loss function and Backpropagation
In order to perform a training, the model requires a loss function $L(y,\hat{y})$.
The loss function $L$ calculates the loss of the model based on the difference of the prediction $\hat{y}$ and the true label $y$.
There exist different loss functions to calculate various types of losses, which are also referred to as error of the model.
Depending on the loss the weights of the model are adjusted, to make a better prediction and reduce the error.
An algorithm called backpropagation is used to perform this adjustment.
The term \textit{learning} refers to this process.

% Generalization error of a model
The goodness of a model can be defined by the performance on the training set $\mathcal{S}$, test set $\mathcal{S}_2$, and validation set $\mathcal{S}_3$.
The two main causes of failure are underfitted or overfitted models.
An underfitted model is not complex enough to understand the interrelations in the data and therefore does not perform well.
Common reasons for underfitted models are missing complexity in the model architecture or too small and not representative datasets.
Overfitted models perform great on the training set $\mathcal{S}$, but poorly on the test set $\mathcal{S}_2$ and validation set $\mathcal{S}_3$.
This happens, when the model does not generalize well enough and instead learns the features of the training set by heart.
To successfully train a model, a tradeoff between overfitting and underfitting must be found.

% Hyperparameter
Further, there exist so-called \textit{hyperparameters}, that are used to adjust the training process or the model architecture.
Important hyperparameters are the learning rate $\lambda$ , the number of training epochs $N_{Epochs}$, and the applied optimizer.


\subsection{Deep Learning}\label{ord:ch2:sec1:subsec2}

A \gls{nn} is one of the standard \gls{ml} methods.
It consists out of multiple neurons, that are connected in a layer-wise architecture.
If an architecture consists out of multiple layers, the network is also described as \textit{deep}, resulting in the terms \gls{dl} and \gls{dnn}.
% A \gls{nn} processes a given input $\textbf{x}$ and makes a prediction $\hat{y}$, similar as a \gls{ml} model.

Nowadays, in order to solve various tasks, a variety of different network types exist \eg convolutional, graph or recurrent neural networks.
For the popular task of image processing \glspl{cnn} have proven themselves to be most adequate to classify, localize, or segment objects in images.
The main component of \glspl{cnn} are convolutional layers, that are used to process and extract features from visual input.
In these layers a convolution operation is performed on the input by applying $c$ convolution kernels.
The output of a layer is defined as a feature map $F_c$, with also $c$ representing the amount of channels in the feature map.
Further, pooling layers are commonly used, in order to reduce the size of feature maps.


\subsection{Evaluation}\label{ord:ch2:sec1:subsec3}

The performance of a model may be evaluated using various metrics.
In the context of classification, the \textit{true positives} are the number of instances, that are correctly predicted to belong to a certain class $c_i$.
While the \textit{false positives} are the number of instances, that are incorrectly predicted to be positive, but truly are negative.
The contradictory definition applies for the equivalent terms \textit{true negatives} and \textit{false negatives}.

% Accuracy
%\begin{equation}
%	Accuracy = \frac{\textnormal{\textit{true positives}} + \textnormal{\textit{true negatives}}}{\textnormal{\textit{true positives}} + \textnormal{\textit{false positives}} + \textnormal{\textit{true negatives}} + \textnormal{\textit{false negatives}}}
% \end{equation}
The accuracy is one of the most simple metrics and represents the number of correct predictions over the number of all predictions.
To further gain deeper insights on the behavior of models there exist more evaluation metrics as precision and recall \cite{Ger17-HandsOn}.
% precision
%The precision is defined by
%\begin{equation}
%	precision = \frac{\textnormal{\textit{true positives}}}{\textnormal{\textit{true positives}} + \textnormal{\textit{false positives}}}
%\end{equation}
%and represents the accuracy of the positive predictions.
% recall
%The recall is defined by
%\begin{equation}
%	recall = \frac{\textnormal{\textit{true positives}}}{\textnormal{\textit{true positives}} + \textnormal{\textit{false negatives}}}
%\end{equation}
%and is also referred to as \textit{sensitivity} or \textit{true positve rate}.
%The recall describes the ratio of positive instances, that are predicted correctly \cite{Ger17-HandsOn}.

% Further, there exist methods for the evaluation of model as the \textit{F1-Score} or the ROC curve, that are described in \cite{Ger17-HandsOn}.